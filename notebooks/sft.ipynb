{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d59874e1",
      "metadata": {
        "id": "d59874e1"
      },
      "source": [
        "# SFT DistilGPT-2 to make Physics Haiku Bot\n",
        "\n",
        "Requires:\n",
        "- train_data.jsonl from data/train/\n",
        "- eval_data.jsonl from data/eval/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dc04175a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc04175a",
        "outputId": "bdbb83f6-1554-469c-f520-f5840611c73e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/518.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# install for Colab use\n",
        "!pip -q install transformers datasets accelerate evaluate trl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daab3904",
      "metadata": {
        "id": "daab3904"
      },
      "source": [
        "First, we create the Datasets for training and evaluation from the pre-formatted, good haiku data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cfc22565",
      "metadata": {
        "id": "cfc22565"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset, DatasetDict, disable_progress_bars\n",
        "\n",
        "def jsonl_to_list(filename):\n",
        "    \"\"\"Helper function to read JSONL into [{}, {}, ... ] where each dict is a haiku sample\"\"\"\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [json.loads(l) for l in f if l.strip()] # ignores whitespace lines\n",
        "\n",
        "ds = DatasetDict(\n",
        "    train = Dataset.from_list(jsonl_to_list(\"train_data.jsonl\")), # train has 1526 good haikus\n",
        "    eval = Dataset.from_list(jsonl_to_list(\"eval_data.jsonl\")), # eval has 157\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb799027",
      "metadata": {
        "id": "cb799027"
      },
      "source": [
        "Next, we tokenize the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea3e59f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea3e59f9",
        "outputId": "87004593-e46f-4f7a-f0e5-b154a40e9783"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from transformers.utils import logging\n",
        "logging.disable_progress_bar() # progress bars don't render well in IDE / GitHub\n",
        "\n",
        "base_name=\"distilgpt2\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(base_name)\n",
        "if tokenizer.pad_token is None: # Trainer needs a pad token for batching\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "def tokenize_and_mask(batch):\n",
        "    # batch is a dict of lists because batched=True\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for prompt, response in zip(batch[\"prompt\"], batch[\"response\"]):\n",
        "        prompt = prompt.strip()\n",
        "        response = response.strip()\n",
        "\n",
        "        # full sequence the model will read\n",
        "        full_text = prompt + \"\\n\" + response + tokenizer.eos_token\n",
        "        # prompt prefix length (we include the newline so the first response token isn't masked)\n",
        "        prompt_text = prompt + \"\\n\"\n",
        "\n",
        "        full_enc = tokenizer(full_text, truncation=True, max_length=256)\n",
        "        prompt_enc = tokenizer(prompt_text, truncation=True, max_length=256)\n",
        "\n",
        "        input_ids = full_enc[\"input_ids\"]\n",
        "        attn = full_enc[\"attention_mask\"]\n",
        "\n",
        "        prompt_len = len(prompt_enc[\"input_ids\"])\n",
        "\n",
        "        # labels = input_ids, but ignore the prompt tokens\n",
        "        labels = input_ids.copy()\n",
        "        for i in range(min(prompt_len, len(labels))):\n",
        "            labels[i] = -100 # Note to self: CrossEntropyLoss 's default ignore_index is -100! thus, only response text contributes to loss\n",
        "\n",
        "        input_ids_list.append(input_ids)\n",
        "        attention_mask_list.append(attn)\n",
        "        labels_list.append(labels)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids_list,\n",
        "        \"attention_mask\": attention_mask_list,\n",
        "        \"labels\": labels_list,\n",
        "    }\n",
        "\n",
        "disable_progress_bars() # progress bars don't render well in IDE / GitHub\n",
        "tok = ds.map(tokenize_and_mask, batched=True, remove_columns=ds[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa45ec59",
      "metadata": {
        "id": "aa45ec59"
      },
      "source": [
        "DataCollatorForLanguageModeling and DataCollatorWithPadding don't easily work since we included loss from only the response section of each haiku's text. The former would overwrite our previously-set prompt ignore labels while the latter throws errors. A custom collator is simpler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "vVU_meKZJge-",
      "metadata": {
        "id": "vVU_meKZJge-"
      },
      "outputs": [],
      "source": [
        "class CollatorPadLabelsToIgnoreIndex:\n",
        "    \"\"\"\n",
        "    Data collator that pads variable-length batches and constructs pad mask/labels manually.\n",
        "    Need it because we construct labels manually above (instead of using DataCollatorForLanguageModeling).\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        max_len = max(len(ex[\"input_ids\"]) for ex in batch) # length of longest sample in batch\n",
        "\n",
        "        input_ids = []\n",
        "        attention_mask = []\n",
        "        labels = []\n",
        "\n",
        "        for ex in batch:\n",
        "            pad_len = max_len - len(ex[\"input_ids\"])\n",
        "            input_ids.append(ex[\"input_ids\"] + [self.tokenizer.pad_token_id] * pad_len)\n",
        "            attention_mask.append(ex[\"attention_mask\"] + [0] * pad_len)\n",
        "            labels.append(ex[\"labels\"] + [-100] * pad_len) # Note to self: CrossEntropyLoss 's default ignore_index is -100!\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype = torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype = torch.long),\n",
        "            \"labels\": torch.tensor(labels, dtype = torch.long),\n",
        "        }\n",
        "\n",
        "collator = CollatorPadLabelsToIgnoreIndex(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdbec0a1",
      "metadata": {
        "id": "cdbec0a1"
      },
      "source": [
        "Finally, we load in the base model, set the training settings, and get to training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b69b5c32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "b69b5c32",
        "outputId": "6d04852c-3114-4f00-ec34-39d489b71a69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='225' max='960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [225/960 01:42 < 05:36, 2.18 it/s, Epoch 2/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.936200</td>\n",
              "      <td>3.056670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.092900</td>\n",
              "      <td>2.901514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>2.910100</td>\n",
              "      <td>2.842354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.850800</td>\n",
              "      <td>2.775371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>2.695700</td>\n",
              "      <td>2.743392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.685100</td>\n",
              "      <td>2.747133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>2.657400</td>\n",
              "      <td>2.720445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.575000</td>\n",
              "      <td>2.724077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>2.517000</td>\n",
              "      <td>2.722208</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=225, training_loss=2.8800079684787327, metrics={'train_runtime': 103.4656, 'train_samples_per_second': 147.489, 'train_steps_per_second': 9.278, 'total_flos': 46009662603264.0, 'train_loss': 2.8800079684787327, 'epoch': 2.345549738219895})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "sft_model = AutoModelForCausalLM.from_pretrained(base_name) # start from DistilGPT-2 with a causal LM head\n",
        "\n",
        "# Align model + generation configs to the tokenizer\n",
        "sft_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "sft_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "sft_model.config.eos_token_id = tokenizer.eos_token_id\n",
        "sft_model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# GPT-2/DistilGPT-2 often uses EOS as BOS; set if needed\n",
        "if tokenizer.bos_token_id is not None:\n",
        "    sft_model.config.bos_token_id = tokenizer.bos_token_id\n",
        "    sft_model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
        "\n",
        "sft_model.to(device)\n",
        "\n",
        "sft_args = TrainingArguments(\n",
        "    output_dir=\"haiku_bot\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=2e-5, # early overfitting for 5e-5\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=25,\n",
        "    save_steps=25,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=25,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\", # don't send logs anywhere\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\"\n",
        ")\n",
        "\n",
        "sft_trainer=Trainer(\n",
        "    model=sft_model,\n",
        "    args=sft_args,\n",
        "    train_dataset=tok[\"train\"],\n",
        "    eval_dataset=tok[\"eval\"],\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=collator,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)] # don't waste free GPU time!\n",
        ")\n",
        "\n",
        "# Let's do some training!\n",
        "sft_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "jkCsnEHD2LVf",
      "metadata": {
        "id": "jkCsnEHD2LVf"
      },
      "outputs": [],
      "source": [
        "save_dir = \"./haiku_bot_final\"\n",
        "sft_trainer.save_model(save_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
